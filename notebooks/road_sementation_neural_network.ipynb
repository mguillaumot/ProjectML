{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import os,sys\n",
    "# from PIL import Image\n",
    "\n",
    "from helpers import *\n",
    "from helpers_NN import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pip\n",
    "\n",
    "try: \n",
    "    import cv2\n",
    "except: \n",
    "    pip.main(['install', 'opencv-python'])\n",
    "    import cv2 \n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except: \n",
    "    pip.main(['install', 'tensorflow'])\n",
    "    import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/images/satImage_021.png\n",
      "Loading training/images/satImage_022.png\n",
      "Loading training/images/satImage_023.png\n",
      "Loading training/images/satImage_024.png\n",
      "Loading training/images/satImage_025.png\n",
      "Loading training/images/satImage_026.png\n",
      "Loading training/images/satImage_027.png\n",
      "Loading training/images/satImage_028.png\n",
      "Loading training/images/satImage_029.png\n",
      "Loading training/images/satImage_030.png\n",
      "Loading training/images/satImage_031.png\n",
      "Loading training/images/satImage_032.png\n",
      "Loading training/images/satImage_033.png\n",
      "Loading training/images/satImage_034.png\n",
      "Loading training/images/satImage_035.png\n",
      "Loading training/images/satImage_036.png\n",
      "Loading training/images/satImage_037.png\n",
      "Loading training/images/satImage_038.png\n",
      "Loading training/images/satImage_039.png\n",
      "Loading training/images/satImage_040.png\n",
      "Loading training/images/satImage_041.png\n",
      "Loading training/images/satImage_042.png\n",
      "Loading training/images/satImage_043.png\n",
      "Loading training/images/satImage_044.png\n",
      "Loading training/images/satImage_045.png\n",
      "Loading training/images/satImage_046.png\n",
      "Loading training/images/satImage_047.png\n",
      "Loading training/images/satImage_048.png\n",
      "Loading training/images/satImage_049.png\n",
      "Loading training/images/satImage_050.png\n",
      "Loading training/images/satImage_051.png\n",
      "Loading training/images/satImage_052.png\n",
      "Loading training/images/satImage_053.png\n",
      "Loading training/images/satImage_054.png\n",
      "Loading training/images/satImage_055.png\n",
      "Loading training/images/satImage_056.png\n",
      "Loading training/images/satImage_057.png\n",
      "Loading training/images/satImage_058.png\n",
      "Loading training/images/satImage_059.png\n",
      "Loading training/images/satImage_060.png\n",
      "Loading training/images/satImage_061.png\n",
      "Loading training/images/satImage_062.png\n",
      "Loading training/images/satImage_063.png\n",
      "Loading training/images/satImage_064.png\n",
      "Loading training/images/satImage_065.png\n",
      "Loading training/images/satImage_066.png\n",
      "Loading training/images/satImage_067.png\n",
      "Loading training/images/satImage_068.png\n",
      "Loading training/images/satImage_069.png\n",
      "Loading training/images/satImage_070.png\n",
      "Loading training/images/satImage_071.png\n",
      "Loading training/images/satImage_072.png\n",
      "Loading training/images/satImage_073.png\n",
      "Loading training/images/satImage_074.png\n",
      "Loading training/images/satImage_075.png\n",
      "Loading training/images/satImage_076.png\n",
      "Loading training/images/satImage_077.png\n",
      "Loading training/images/satImage_078.png\n",
      "Loading training/images/satImage_079.png\n",
      "Loading training/images/satImage_080.png\n",
      "Loading training/images/satImage_081.png\n",
      "Loading training/images/satImage_082.png\n",
      "Loading training/images/satImage_083.png\n",
      "Loading training/images/satImage_084.png\n",
      "Loading training/images/satImage_085.png\n",
      "Loading training/images/satImage_086.png\n",
      "Loading training/images/satImage_087.png\n",
      "Loading training/images/satImage_088.png\n",
      "Loading training/images/satImage_089.png\n",
      "Loading training/images/satImage_090.png\n",
      "Loading training/images/satImage_091.png\n",
      "Loading training/images/satImage_092.png\n",
      "Loading training/images/satImage_093.png\n",
      "Loading training/images/satImage_094.png\n",
      "Loading training/images/satImage_095.png\n",
      "Loading training/images/satImage_096.png\n",
      "Loading training/images/satImage_097.png\n",
      "Loading training/images/satImage_098.png\n",
      "Loading training/images/satImage_099.png\n",
      "Loading training/images/satImage_100.png\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "all_train_data = extract_data_train(train_data_filename, TRAINING_SIZE)\n",
    "all_train_labels = extract_labels(train_labels_filename, TRAINING_SIZE, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_train_data.shape)\n",
    "print(all_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize data before using them\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "all_train_data = np.random.permutation(all_train_data)\n",
    "all_train_labels = np.random.permutation(all_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c0, c1 = 0, 0\n",
    "for i in range(len(all_train_labels)):\n",
    "    if all_train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing training data...\n",
      "2050112\n",
      "(4000000, 10)\n",
      "Number of data points per class: c0 = 1025056 c1 = 1025056\n"
     ]
    }
   ],
   "source": [
    "print ('Balancing training data...')\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(all_train_labels) if j[0] == 1]\n",
    "idx1 = [i for i, j in enumerate(all_train_labels) if j[1] == 1]\n",
    "new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "print (len(new_indices))\n",
    "print (all_train_data.shape)\n",
    "all_train_data = all_train_data[new_indices,:]\n",
    "all_train_labels = all_train_labels[new_indices]\n",
    "\n",
    "\n",
    "train_size = all_train_labels.shape[0]\n",
    "\n",
    "c0, c1 = 0, 0\n",
    "for i in range(len(all_train_labels)):\n",
    "    if all_train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(train_size)\n",
    "all_train_data = all_train_data[p,:]\n",
    "all_train_labels = all_train_labels[p,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network parameters\n",
    "nb_hidden_nodes_1 = 500\n",
    "nb_hidden_nodes_2 = 500\n",
    "NUM_HIDDEN_LAYERS = 1\n",
    "NUM_PATCH = train_size\n",
    "NUM_FEATURES = all_train_data.shape[1]\n",
    "NUM_LABELS = all_train_labels.shape[1]\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_STEPS = 3001\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 100)\n",
    "\n",
    "for ind, [train_index, test_index] in enumerate(kf.split(all_train_data)):\n",
    "    data_train_cv, data_test_cv = all_train_data[train_index], all_train_data[test_index]\n",
    "    labels_train_cv, labels_test_cv = all_train_labels[train_index], all_train_labels[test_index]\n",
    "    \n",
    "train_dataset = data_train_cv\n",
    "train_labels = labels_train_cv\n",
    "\n",
    "valid_dataset = data_test_cv\n",
    "valid_labels = labels_test_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = all_train_data\n",
    "# train_labels = all_train_labels\n",
    "\n",
    "# valid_dataset = data_test_cv\n",
    "# valid_labels = labels_test_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test_set_images/test_1/test_1.png\n",
      "Loading test_set_images/test_10/test_10.png\n",
      "Loading test_set_images/test_11/test_11.png\n",
      "Loading test_set_images/test_12/test_12.png\n",
      "Loading test_set_images/test_13/test_13.png\n",
      "Loading test_set_images/test_14/test_14.png\n",
      "Loading test_set_images/test_15/test_15.png\n",
      "Loading test_set_images/test_16/test_16.png\n",
      "Loading test_set_images/test_17/test_17.png\n",
      "Loading test_set_images/test_18/test_18.png\n",
      "Loading test_set_images/test_19/test_19.png\n",
      "Loading test_set_images/test_2/test_2.png\n",
      "Loading test_set_images/test_20/test_20.png\n",
      "Loading test_set_images/test_21/test_21.png\n",
      "Loading test_set_images/test_22/test_22.png\n",
      "Loading test_set_images/test_23/test_23.png\n",
      "Loading test_set_images/test_24/test_24.png\n",
      "Loading test_set_images/test_25/test_25.png\n",
      "Loading test_set_images/test_26/test_26.png\n",
      "Loading test_set_images/test_27/test_27.png\n",
      "Loading test_set_images/test_28/test_28.png\n",
      "Loading test_set_images/test_29/test_29.png\n",
      "Loading test_set_images/test_3/test_3.png\n",
      "Loading test_set_images/test_30/test_30.png\n",
      "Loading test_set_images/test_31/test_31.png\n",
      "Loading test_set_images/test_32/test_32.png\n",
      "Loading test_set_images/test_33/test_33.png\n",
      "Loading test_set_images/test_34/test_34.png\n",
      "Loading test_set_images/test_35/test_35.png\n",
      "Loading test_set_images/test_36/test_36.png\n",
      "Loading test_set_images/test_37/test_37.png\n",
      "Loading test_set_images/test_38/test_38.png\n",
      "Loading test_set_images/test_39/test_39.png\n",
      "Loading test_set_images/test_4/test_4.png\n",
      "Loading test_set_images/test_40/test_40.png\n",
      "Loading test_set_images/test_41/test_41.png\n",
      "Loading test_set_images/test_42/test_42.png\n",
      "Loading test_set_images/test_43/test_43.png\n",
      "Loading test_set_images/test_44/test_44.png\n",
      "Loading test_set_images/test_45/test_45.png\n",
      "Loading test_set_images/test_46/test_46.png\n",
      "Loading test_set_images/test_47/test_47.png\n",
      "Loading test_set_images/test_48/test_48.png\n",
      "Loading test_set_images/test_49/test_49.png\n",
      "Loading test_set_images/test_5/test_5.png\n",
      "Loading test_set_images/test_50/test_50.png\n",
      "Loading test_set_images/test_6/test_6.png\n",
      "Loading test_set_images/test_7/test_7.png\n",
      "Loading test_set_images/test_8/test_8.png\n",
      "Loading test_set_images/test_9/test_9.png\n"
     ]
    }
   ],
   "source": [
    "# Data to evaluate\n",
    "root_testdir = \"test_set_images\"\n",
    "test_names = os.listdir(root_testdir)\n",
    "\n",
    "IMG_TEST_SIZE = len(test_names)\n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "test_dataset = extract_data_test(root_testdir, IMG_TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20501, 10)\n",
      "(2029611, 10)\n",
      "(72200, 10)\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset.shape)\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \"\"\" Performance measures of the network\n",
    "        Computation of :\n",
    "        - Accuracy\n",
    "        - Precision (PPV)\n",
    "        - Sensitivity (TPR)\n",
    "        - Confusion matrix\n",
    "    \"\"\"\n",
    "    # Accuracy\n",
    "    accuracy = (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\prisgdd\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized!\n",
      "EPOCH :: 0\n",
      "\n",
      "Minibatch loss at step 0: 1.224394\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.1%\n",
      "\n",
      "Minibatch loss at step 100: 0.709770\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Minibatch loss at step 200: 0.896669\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Minibatch loss at step 300: 0.722460\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 50.4%\n",
      "\n",
      "Minibatch loss at step 400: 0.715565\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 50.1%\n",
      "\n",
      "Minibatch loss at step 500: 0.701525\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 49.8%\n",
      "\n",
      "Minibatch loss at step 600: 0.675718\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Minibatch loss at step 700: 0.677330\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 49.8%\n",
      "\n",
      "Minibatch loss at step 800: 0.666614\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 49.6%\n",
      "\n",
      "Minibatch loss at step 900: 0.664855\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Minibatch loss at step 1000: 0.705384\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.4%\n",
      "\n",
      "Minibatch loss at step 1100: 0.712151\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 50.3%\n",
      "\n",
      "Minibatch loss at step 1200: 0.723213\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 50.0%\n",
      "\n",
      "Minibatch loss at step 1300: 0.711952\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 49.8%\n",
      "\n",
      "Minibatch loss at step 1400: 0.716570\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 50.1%\n",
      "\n",
      "Minibatch loss at step 1500: 0.698684\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 50.1%\n",
      "\n",
      "Minibatch loss at step 1600: 0.711364\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 50.0%\n",
      "\n",
      "Minibatch loss at step 1700: 0.707211\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 49.8%\n",
      "\n",
      "Minibatch loss at step 1800: 0.722768\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 50.3%\n",
      "\n",
      "Minibatch loss at step 1900: 0.708626\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 49.8%\n",
      "\n",
      "Minibatch loss at step 2000: 0.694892\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 49.8%\n",
      "\n",
      "Minibatch loss at step 2100: 0.696324\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 50.0%\n",
      "\n",
      "Minibatch loss at step 2200: 0.711772\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Minibatch loss at step 2300: 0.707852\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.1%\n",
      "\n",
      "Minibatch loss at step 2400: 0.722529\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Minibatch loss at step 2500: 0.695523\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.0%\n",
      "\n",
      "Minibatch loss at step 2600: 0.730557\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 50.0%\n",
      "\n",
      "Minibatch loss at step 2700: 0.707811\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 49.7%\n",
      "\n",
      "Minibatch loss at step 2800: 0.698430\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Minibatch loss at step 2900: 0.699751\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.2%\n",
      "\n",
      "Minibatch loss at step 3000: 0.705876\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 49.9%\n",
      "\n",
      "Test prediction done!\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('Inputs_management'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_FEATURES), name='tf_train_dataset')\n",
    "        tf_train_labels = tf.placeholder(tf.int32, shape=(BATCH_SIZE, NUM_LABELS), name='tf_train_labels')\n",
    "\n",
    "        tf_valid_dataset = tf.constant(valid_dataset, name=\"tf_valid_dataset\")\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Weights and bias.\n",
    "    with tf.name_scope('Weights_bias_management'):                              \n",
    "        def weight_variable(shape, name=None):\n",
    "            \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "            return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name = name)\n",
    "\n",
    "\n",
    "        def bias_variable(shape, name=None):\n",
    "            \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "            return tf.Variable(tf.constant(0.1, shape=shape),name = name)\n",
    "\n",
    "        if NUM_HIDDEN_LAYERS == 1:\n",
    "             # Weights and bias for the first layer\n",
    "            W_fc1 = weight_variable([NUM_FEATURES, nb_hidden_nodes_1], \"W_fc1\")\n",
    "            b_fc1 = bias_variable([nb_hidden_nodes_1],\"b_fc1\")\n",
    "\n",
    "            # Weights and bias for the second layer\n",
    "            W_fc2 = weight_variable([nb_hidden_nodes_1, NUM_LABELS], \"W_fc2\")\n",
    "            b_fc2 = bias_variable([NUM_LABELS],\"b_fc2\")\n",
    "\n",
    "        elif NUM_HIDDEN_LAYERS == 2:\n",
    "            # Weights and bias for the first layer\n",
    "            W_fc1 = weight_variable([NUM_FEATURES, nb_hidden_nodes_1], \"W_fc1\")\n",
    "            b_fc1 = bias_variable([nb_hidden_nodes_1],\"b_fc1\")\n",
    "\n",
    "            # Weights and bias for the second layer\n",
    "            W_fc2 = weight_variable([nb_hidden_nodes_1, nb_hidden_nodes_2], \"W_fc2\")\n",
    "            b_fc2 = bias_variable([nb_hidden_nodes_2],\"b_fc2\")\n",
    "\n",
    "            # Weights and bias for the third layer\n",
    "            W_fc3 = weight_variable([nb_hidden_nodes_2, NUM_LABELS], \"W_fc3\")\n",
    "            b_fc3 = bias_variable([NUM_LABELS],\"b_fc3\")\n",
    "\n",
    "    \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        if NUM_HIDDEN_LAYERS == 1:\n",
    "            # First layer\n",
    "            with tf.name_scope('FullyConnected1'):\n",
    "                h_fc1 = tf.matmul(data, W_fc1) + b_fc1\n",
    "                h_relu1 = tf.nn.relu(h_fc1)\n",
    "\n",
    "            # Second Layre\n",
    "            with tf.name_scope('FullyConnected2'):\n",
    "                h_fc2 = tf.matmul(h_relu1, W_fc2) + b_fc2\n",
    "                valren = h_fc2\n",
    "\n",
    "        elif NUM_HIDDEN_LAYERS == 2:\n",
    "            # First layer\n",
    "            with tf.name_scope('FullyConnected1'):\n",
    "                h_fc1 = tf.matmul(data, W_fc1) + b_fc1\n",
    "                h_relu1 = tf.nn.relu(h_fc1)\n",
    "\n",
    "            # Second Layre\n",
    "            with tf.name_scope('FullyConnected2'):\n",
    "                h_fc2 = tf.matmul(h_relu1, W_fc2) + b_fc2\n",
    "                h_relu2 = tf.nn.relu(h_fc2)\n",
    "\n",
    "            # third Layer\n",
    "            with tf.name_scope('FullyConnected3'):\n",
    "                h_fc3 = tf.matmul(h_relu2, W_fc3) + b_fc3\n",
    "                valren = h_fc3\n",
    "\n",
    "        return valren \n",
    "\n",
    "    # Weights and bias.\n",
    "    with tf.name_scope('Training_computations'):      \n",
    "        logits = model(tf_train_dataset)\n",
    "    \n",
    "    with tf.name_scope('Loss_computation'):\n",
    "        def compute_loss(logits, tf_train_labels, lambda_reg = 5e-4, regularization = True):\n",
    "            if not regularization:\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "            else:\n",
    "                reg = lambda_reg * (tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(W_fc2))\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels =  tf_train_labels) + reg)\n",
    "            return loss\n",
    "\n",
    "        loss = compute_loss(logits, tf_train_labels)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    with tf.name_scope('Optimization'):\n",
    "        \n",
    "        # Optimizer: set up a variable that's incremented once per batch and\n",
    "        # controls the learning rate decay.\n",
    "        batch = tf.Variable(0)\n",
    "        # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            0.01,                # Base learning rate.\n",
    "            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "            train_size,          # Decay step.\n",
    "            0.95,                # Decay rate.\n",
    "            staircase=True)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "        # Use simple momentum for the optimization.\n",
    "#         optimizer = tf.train.MomentumOptimizer(learning_rate, 0.0).minimize(loss, global_step=batch)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "#     saver = tf.train.Saver(weightsDict)\n",
    "\n",
    "\n",
    "    with tf.name_scope('Predictions'):\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(model(tf_valid_dataset), name=\"valid_prediction\")\n",
    "\n",
    "#         data_pred = tf.nn.softmax(classifier.model(tf_data, weightsDict)[0], name=\"output\")\n",
    "        test_prediction = tf.nn.softmax(model(tf_test_dataset)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized!')\n",
    "    for epoch in range(0, NUM_EPOCHS):\n",
    "        print(\"EPOCH :: %d\" % (epoch))\n",
    "        for step in range(NUM_STEPS):\n",
    "            offset = (step * BATCH_SIZE) % (train_dataset.shape[0] - BATCH_SIZE)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + BATCH_SIZE), :]\n",
    "            batch_labels = train_labels[offset:(offset + BATCH_SIZE), :]\n",
    "\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob:0.7}\n",
    "            _, l, predictions, summary = session.run([optimizer, loss, train_prediction, summary_op], feed_dict=feed_dict)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('\\nMinibatch loss at step %d: %f' % (step, l))\n",
    "\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(labels = batch_labels, predictions = predictions))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(labels = valid_labels, predictions = valid_prediction.eval(feed_dict = {keep_prob:1.0})))\n",
    "            \n",
    "            \n",
    "    test_pred = test_prediction.eval()\n",
    "    print(\"\\nTest prediction done!\")\n",
    "#     print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [np.argmax(p) for p in test_pred]\n",
    "create_submission(y_pred, \"submission_NN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
